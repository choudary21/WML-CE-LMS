{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Tensorflow Large Model Support (TFLMS) Tutorial - 02</center></h1>\n",
    "\n",
    "## Large Model Support\n",
    "\n",
    "TensorFlow Large Model Support (TFLMS) is a Python module that provides an approach to training large models and data that cannot normally be fit in to GPU memory. It takes a computational graph defined by users, and automatically adds swap-in and swap-out nodes for transferring tensors from GPUs to the host and vice versa. During training and inferencing this makes the graph execution operate like operating system memory paging. The system memory is effectively treated as a paging cache for the GPU memory and tensors are swapped back and forth between the GPU memory and CPU memory.\n",
    "\n",
    "Click <a href= https://www.ibm.com/support/knowledgecenter/en/SS5SF7_1.5.4/navigation/pai_tflms.html>here</a> for further information on how to use TFLMS and best practices.\n",
    "\n",
    "TFLMS source code is publicly available as a pull request in the <a href= https://github.com/tensorflow/tensorflow/pull/19845>TensorFlow repository</a>.\n",
    "\n",
    "Here are links to blog posts, papers, and videos that describe TensorFlow Large Model support, use cases, and performance characteristics.\n",
    "\n",
    "1) <a href = https://arxiv.org/pdf/1807.02037.pdf> TFLMS: Large Model Support in TensorFlow by Graph Rewriting here </a>\n",
    "\n",
    "2) <a href = https://www.youtube.com/watch?vwdVPh3tUQ5A/> 4 minute introduction to TensorFlow Large Model support </a> – This video is a good quick introduction to TensorFlow Large Model Support. Note that the performance numbers at the end of this video are now out dated. See the performance links below for updated performance numbers.\n",
    "\n",
    "3) <a href = https://on-demand-gtc.gputechconf.com/gtcnew/sessionview.php?sessionName=s9426-using+tensor+swapping+and+nvlink+to+overcome+gpu+memory+limits+with+tensorflow/> NVIDIA GPU Technology Conference 2019 presentation </a> – A 40 minute presentation that discusses the use of TFLMS to overcome GPU memory limits and performance characteristics of TFLMS.\n",
    "\n",
    "4) <a href = https://developer.ibm.com/linuxonpower/2019/05/17/performance-results-with-tensorflow-large-model-support-v2/> Performance results with TensorFlow Large Model Support v2 </a>\n",
    "\n",
    "5) <a href = https://developer.ibm.com/linuxonpower/2018/07/27/tensorflow-large-model-support-case-study-3d-image-segmentation/> A case study using TensorFlow Large Model Support with 3D U-Net for 3D image segmentation </a>\n",
    "\n",
    "6) <a href = https://arxiv.org/abs/1812.07816> Fast and Accurate 3D Medical Image Segmentation with Data-swapping Method </a> – This paper contains a comparison of using TFLMS versus patching method for large images. It also contains a comparison of TFLMS vs gradient checkpointing.\n",
    "\n",
    "7) <a href = https://arxiv.org/abs/1811.12174> Data-parallel distributed training of very large models beyond GPU capacity </a> – This paper contains a real world use case of using TFLMS with IBM Distributed Deep Learning.\n",
    "\n",
    "8) <a href = https://developer.ibm.com/linuxonpower/2018/12/19/performance-of-3dunet-multi-gpu-model-for-medical-image-segmentation-using-tensorflow-large-model-support/> Performance of 3DUnet Multi GPU Model for Medical Image Segmentation using TensorFlow Large Model Support </a> – This blog post contains performance comparisons of whole system training using TFLMS with IBM Distributed Deep Learning and Horovod on x86 and IBM AC922 servers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Classification using TFLMS and Keras\n",
    "\n",
    "In this example we will train a CNN on MNIST dataset, using Tensorflow Large Model Support along with Keras. \n",
    "\n",
    "<b>Note: This example just demonstrates how to deploy LMS into Keras applications. The program runs as well without TFLMS. The model itself is not big enough to necessitate TFLMS.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.python.keras.datasets import mnist\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.python.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "from tensorflow_large_model_support import LMS\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "Load the MNIST dataset and separate into training set and testing set.\n",
    "\n",
    "\n",
    "## MNIST Dataset Overview\n",
    "\n",
    "This example is using MNIST handwritten digits. The dataset contains 60,000 examples for training and 10,000 examples for testing. The digits have been size-normalized and centered in a fixed-size image (28x28 pixels) with values from 0 to 255. \n",
    "\n",
    "In this example, each image will be converted to float32 and normalized to [0, 1].\n",
    "\n",
    "![MNIST Dataset](http://neuralnetworksanddeeplearning.com/images/mnist_100_digits.png)\n",
    "\n",
    "More info: http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "x_test shape: (10000, 28, 28, 1)\n",
      "10000 test samples\n",
      "y_train shape: (60000, 10)\n",
      "60000 train labels\n",
      "y_test shape: (10000, 10)\n",
      "10000 test labels\n"
     ]
    }
   ],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "batch_size = 128\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print('x_test shape:', x_test.shape)\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print('y_train shape:', y_train.shape)\n",
    "print(y_train.shape[0], 'train labels')\n",
    "print('y_test shape:', y_test.shape)\n",
    "print(y_test.shape[0], 'test labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## CNN Overview\n",
    "\n",
    "![CNN](http://personal.ie.cuhk.edu.hk/~ccloy/project_target_code/images/fig3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LMS( ) is the Keras callback to activate Large Model Support. If we do not specify specific tuning parameters to LMS( ), the auto tuning will determine that TFLMS is not needed and disable it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = createModel()\n",
    "ms_callback = LMS(swapout_threshold=40, swapin_ahead=3, swapin_groupby=2)\n",
    "\n",
    "#Run \"watch -n0.1 nvidia-smi\" to see the GPU utilization\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=2,\n",
    "          validation_data=(x_test, y_test),\n",
    "          callbacks=[lms_callback])\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
